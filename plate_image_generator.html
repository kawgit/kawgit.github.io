<!DOCTYPE HTML>
<html>
	<head>
		<title>Kenneth Wilber • Programming Portfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body>
		<div id="wrapper">
			<header id="header" style="padding-top: 60px; text-align:center;">
				<div class="inner">
					<a href="index.html"> <h3 style="color: white; font-size: larger;">Kenneth Wilber • Programming Portfolio</h3></a>
				</div>
			</header>
			<div id="main">
				<div class="inner">
					<h1 style="text-align: center;">Text Generator</h1>							  
					<span class="image main">
						<video width="720" height="720" autoplay muted loop style="margin: auto; display: block;">
							<source src="videos/text_generation.mp4" type="video/mp4"/>
						</video>
						<p style="text-align: center;">The above video shows my project generating text given the seed prompt <br> "I believe the meaning of life is," although I admit the output is nonsensical <br> and instead resembles the Shakespearean plays the model was trained on. </h4>
					</span>
					<h3>What it does</h3>
					<p>A command line program to generate text trained on a dataset of Shakespeare plays using a small-scale version of the very same “self-attention” technique used by popular tools like ChatGPT and Bard.</p>
					<h3>Tools used</h3>
					<ul>
						<li>Python</li>
						<li>Pytorch</li>
						<li>Jupyter Notebooks</li>
						<li>CUDA</li>
					</ul>
					<h3>Links</h3>
					<a href="https://github.com/kawgit/Vision-Transformer"><img src="images\link_icon.png" style="width:20px; height:auto; display:inline;">  Source code</a>
					<br>
					<br>
					<h3>How it works</h3>
					<p>The tool generates text by predicting the next token (a character or word) in a sequence of tokens.</p>
					<p>For each token in the input sequence, three vectors of equal length are generated by a trainable model, respectively called the token’s “key,” and “query,” and “output” vectors.  In essence, the “query” of the “query token” is what that token is interested in. The “key” of the “key token” is what information that token has to offer.</p>
					<p>For every token, dot its query vector with the key vector of every other token in the input sequence. The result of each dot operation is a scalar representing the “interest level” that the “query token” should have in the “key token”.</p>
					<p>When dotting two vectors, you perform an element-wise multiplication and then sum up the resulting values. The ultimate result will only be high if the “highs” of the query vector lined up with the “highs” of the key vector. For example, if the query={4, .1} and the key={1, 7}, then the resulting dot product, 4.7, would indicate a low “interest level” compared to if the query={.1, 4} and the key={1, 7} which results in a dot produce of 28.1.</p>
					<p>Once you have calculated a level of interest for every permutation of two tokens in the sequence, create a new sequence of the same length as the input sequence but filled with feature vectors which are, for each token, the sum of the output vectors of each of the input tokens weighted by the “interest level.” This technique allows tokens coming from an arbitrarily sized input sequence to communicate with each other and learn relationships which ultimately help predict the next token in the input sequence.</p>
				</div>
			</div>
		</div>

		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>

	</body>
</html>